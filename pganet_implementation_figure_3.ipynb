{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "45e3a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "import uuid\n",
    "import cvxpy as cp\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from scipy.io import loadmat\n",
    "import h5py\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "593a49b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define System and Simulation Parameters\n",
    "N = 64  # Number of BS antennas\n",
    "K = 4   # Number of users\n",
    "M = 4   # Number of RF chains\n",
    "omega = 0.3  # Tradeoff weight\n",
    "I_max = 120  # Maximum outer iterations\n",
    "J = 10  # Can be 1, 10, or 20\n",
    "\n",
    "SNR_dB = 12  # SNR in dB\n",
    "sigma_n2 = 1  # Noise variance\n",
    "P_BS = sigma_n2 * 10**(SNR_dB / 10)  # Transmit power\n",
    "mu = 0.01  # Step size for analog precoder\n",
    "lambda_ = 0.01  # Step size for digital precoder\n",
    "L = 20  # Number of paths for channel\n",
    "num_realizations = 100  # Number of channel realizations\n",
    "\n",
    "\n",
    "# Dataset parameters\n",
    "num_channels = 100\n",
    "num_epochs = 100 if J == 100 else 30\n",
    "snr_min, snr_max = 0, 12  # dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1005b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def to_tensor(x, dtype=None, device=None):\n",
    "    \"\"\"Convert numpy or tensor input to a torch tensor on the right device.\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.to(device=device or 'cpu', dtype=dtype)\n",
    "    elif isinstance(x, np.ndarray):\n",
    "        return torch.as_tensor(x, dtype=dtype, device=device)\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported type: {type(x)}\")\n",
    "\n",
    "def to_numpy(x):\n",
    "    \"\"\"Convert tensor to numpy array (CPU)\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().numpy()\n",
    "    elif isinstance(x, np.ndarray):\n",
    "        return x\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported type: {type(x)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "867e246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define Sensing Parameters\n",
    "P = 3  # Number of desired sensing angles\n",
    "theta_d = np.array([-60, 0, 60]) * np.pi / 180  # Desired angles in radians\n",
    "delta_theta = 5 * np.pi / 180  # Half beamwidth\n",
    "theta_grid = np.linspace(-np.pi / 2, np.pi / 2, 181)  # Angular grid [-90, 90] degrees\n",
    "B_d = np.zeros(len(theta_grid))  # Desired beampattern\n",
    "for t, theta_t in enumerate(theta_grid):\n",
    "    for theta_p in theta_d:\n",
    "        if abs(theta_t - theta_p) <= delta_theta:\n",
    "            B_d[t] = 1\n",
    "\n",
    "# Wavenumber and antenna spacing\n",
    "lambda_wave = 1  # Wavelength (normalized)\n",
    "k = 2 * np.pi / lambda_wave\n",
    "d = lambda_wave / 2  # Antenna spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d6ee6d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Step 3: Channel Matrix Generation (Saleh-Valenzuela Model)\n",
    "def generate_channel(N, M, L):\n",
    "    H = np.zeros((M, N), dtype=complex)\n",
    "    for _ in range(L):\n",
    "        alpha = np.random.normal(0, 1, 2).view(complex)[0] / np.sqrt(2)  # Complex gain\n",
    "        phi_r = np.random.uniform(0, 2 * np.pi)  # Angle of arrival\n",
    "        phi_t = np.random.uniform(0, 2 * np.pi)  # Angle of departure\n",
    "        a_r = np.exp(1j * k * d * np.arange(M) * np.sin(phi_r)) / np.sqrt(M)\n",
    "        a_t = np.exp(1j * k * d * np.arange(N) * np.sin(phi_t)) / np.sqrt(N)\n",
    "        H += np.sqrt(N * M / L) * alpha * np.outer(a_r, a_t.conj())\n",
    "    return H\n",
    "\n",
    "# Steering vector function\n",
    "def steering_vector(theta, N):\n",
    "    return np.exp(1j * k * d * np.arange(N) * np.sin(theta)) / np.sqrt(N)\n",
    "\n",
    "\n",
    "# Compute communication rate R\n",
    "def compute_rate(H, A, D, sigma_n2):\n",
    "    H_A = H @ A  # Effective channel\n",
    "    R = 0\n",
    "    for k in range(K):\n",
    "        h_k = H_A[:, k]\n",
    "        \n",
    "        signal = torch.abs(h_k.conj().T @ D[:, k])**2\n",
    "        interference = sum(torch.abs(h_k.conj().T @ D[:, j])**2 for j in range(K) if j != k)\n",
    "        SINR = signal / (interference + sigma_n2)\n",
    "        R += torch.log2(1 + SINR)\n",
    "    return R\n",
    "\n",
    "# Compute sensing error tau\n",
    "def compute_tau(A, D, Psi, theta_grid):\n",
    "    V = A @ D\n",
    "    tau = 0\n",
    "    for theta in theta_grid:\n",
    "        a_theta = steering_vector(theta, N)\n",
    "        # convert a_theta to torch tensor\n",
    "        a_theta = torch.tensor(a_theta, dtype=torch.cfloat)\n",
    "        tau += torch.abs(a_theta.conj().T @ V @ V.conj().T @ a_theta - a_theta.conj().T @ Psi @ a_theta)**2\n",
    "    return tau / len(theta_grid)\n",
    "\n",
    "def gradient_R_A(H, A, D, sigma_n2):\n",
    "    xi = 1 / torch.log(torch.tensor(2.0, dtype=A.dtype, device=A.device))\n",
    "    grad_A = torch.zeros_like(A, dtype=torch.cfloat)\n",
    "\n",
    "    # Effective covariance of the digital precoder\n",
    "    V = D @ D.conj().transpose(-2, -1)\n",
    "\n",
    "    for k in range(K):\n",
    "        # User-k effective channel outer product\n",
    "        h_k = H[k, :].reshape(-1, 1)  # (M x 1)\n",
    "        H_tilde_k = h_k @ h_k.conj().transpose(-2, -1)\n",
    "\n",
    "        # D_bar_k = D with user k's column set to zero\n",
    "        D_bar_k = D.clone()\n",
    "        D_bar_k[:, k] = 0.0\n",
    "        V_bar_k = D_bar_k @ D_bar_k.conj().transpose(-2, -1)\n",
    "\n",
    "        # Denominator terms (trace parts)\n",
    "        denom1 = torch.trace(A @ V @ A.conj().transpose(-2, -1) @ H_tilde_k) + sigma_n2\n",
    "        denom2 = torch.trace(A @ V_bar_k @ A.conj().transpose(-2, -1) @ H_tilde_k) + sigma_n2\n",
    "\n",
    "        # Gradient contribution\n",
    "        term1 = H_tilde_k @ A @ V / denom1\n",
    "        term2 = H_tilde_k @ A @ V_bar_k / denom2\n",
    "\n",
    "        grad_A += xi * (term1 - term2)\n",
    "\n",
    "    return grad_A\n",
    "\n",
    "def gradient_R_D(H, A, D, sigma_n2):\n",
    "    xi = 1 / torch.log(torch.tensor(2.0, dtype=A.dtype, device=A.device))\n",
    "    grad_D = torch.zeros_like(D, dtype=torch.cfloat)\n",
    "\n",
    "    for k in range(K):\n",
    "        # Channel vector for user k\n",
    "        h_k = H[k, :].reshape(-1, 1)  # (N x 1)\n",
    "        H_tilde_k = h_k @ h_k.conj().transpose(-2, -1)\n",
    "\n",
    "        # Effective digital-domain channel including analog precoder\n",
    "        H_bar_k = A.conj().transpose(-2, -1) @ H_tilde_k @ A\n",
    "\n",
    "        # D_bar_k = D with k-th column set to zero\n",
    "        D_bar_k = D.clone()\n",
    "        D_bar_k[:, k] = 0.0\n",
    "\n",
    "        # Compute denominator terms (trace parts)\n",
    "        denom1 = torch.trace(D @ D.conj().transpose(-2, -1) @ H_bar_k) + sigma_n2\n",
    "        denom2 = torch.trace(D_bar_k @ D_bar_k.conj().transpose(-2, -1) @ H_bar_k) + sigma_n2\n",
    "\n",
    "        # Compute gradient contributions\n",
    "        term1 = (H_bar_k @ D) / denom1\n",
    "        term2 = (H_bar_k @ D_bar_k) / denom2\n",
    "\n",
    "        grad_D += xi * (term1 - term2)\n",
    "\n",
    "    return grad_D\n",
    "\n",
    "def gradient_tau_A(A, D, Psi):\n",
    "    U = A @ D @ D.conj().transpose(-2, -1) @ A.conj().transpose(-2, -1)  # A D D^H A^H\n",
    "    grad_A = 2 * (U - Psi) @ A @ D @ D.conj().transpose(-2, -1)\n",
    "    return grad_A\n",
    "\n",
    "def gradient_tau_D(A, D, Psi):\n",
    "    U = A @ D @ D.conj().transpose(-2, -1) @ A.conj().transpose(-2, -1)  # A D D^H A^H\n",
    "    grad_D = 2 * A.conj().transpose(-2, -1) @ (U - Psi) @ A @ D\n",
    "    return grad_D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "31fd9199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Psi_all shape: (121, 64, 64)\n",
      "Psi_all final shape: (121, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with h5py.File('Psi_all.mat', 'r') as f:\n",
    "    Psi_all = np.array(f['Psi_all'], dtype=np.complex128)  # force numeric type\n",
    "    SNR_dB = np.array(f['SNR_dB']).flatten()\n",
    "\n",
    "    print(\"Raw Psi_all shape:\", Psi_all.shape)\n",
    "\n",
    "    # Optional: squeeze if necessary (depends on dimensions)\n",
    "    if Psi_all.ndim == 4:\n",
    "        Psi_all = np.squeeze(Psi_all, axis=2)\n",
    "    \n",
    "    print(\"Psi_all final shape:\", Psi_all.shape)\n",
    "\n",
    "\n",
    "\n",
    "def compute_psi(snr_db):\n",
    "    idx = np.argmin(np.abs(SNR_dB - snr_db))\n",
    "    Psi = Psi_all[idx, :, :]\n",
    "    return Psi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "92f9f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_unit_modulus(A):\n",
    "    return torch.exp(1j * torch.angle(A))\n",
    "\n",
    "def project_power_constraint(A, D, P_BS):\n",
    "    norm_factor = torch.norm(A @ D, 'fro')\n",
    "    D = D * (torch.sqrt(P_BS) / norm_factor)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d0679ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def proposed_initialization(H, theta_d, N, M, K, P_BS):\n",
    "    G = np.array([H[k, :] for k in range(K)]).T  # N x K\n",
    "    A0 = np.exp(1j * np.angle(G))\n",
    "    X_ZF = np.linalg.pinv(H)\n",
    "    D0 = np.linalg.pinv(A0) @ X_ZF\n",
    "    D0 = np.sqrt(P_BS) * D0 / np.linalg.norm(A0 @ D0, 'fro')\n",
    "    return A0, D0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "8d6cf8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UPGANetLayer(nn.Module):\n",
    "    def __init__(self, N, M, K, omega, J=10, eta=None):\n",
    "        super(UPGANetLayer, self).__init__()\n",
    "        self.J = J\n",
    "        self.N, self.M, self.K = N, M, K\n",
    "        self.omega = omega\n",
    "        self.eta = eta if eta is not None else 1/N\n",
    "\n",
    "        # Learnable step sizes (initialized to μ(0,0)=λ(0)=0.01)\n",
    "        self.mu = nn.Parameter(torch.full((J,), 0.01, dtype=torch.float32))\n",
    "        self.lambda_ = nn.Parameter(torch.tensor(0.01, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, H, A, D, Psi, sigma_n2, P_BS):\n",
    "        # --- J inner updates for analog precoder ---\n",
    "        for j in range(self.J):\n",
    "            grad_RA = gradient_R_A(H, A, D, sigma_n2)\n",
    "            grad_tauA = gradient_tau_A(A, D, Psi)\n",
    "            A = A + self.mu[j] * (grad_RA - self.omega * grad_tauA)\n",
    "            A = project_unit_modulus(A)\n",
    "\n",
    "        # --- Digital precoder update ---\n",
    "        grad_RD = gradient_R_D(H, A, D, sigma_n2)\n",
    "        grad_tauD = gradient_tau_D(A, D, Psi)\n",
    "        D = D + self.lambda_ * (grad_RD - self.omega * self.eta * grad_tauD)\n",
    "        D = project_power_constraint(A, D, P_BS)\n",
    "\n",
    "        return A, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "858143a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UPGANet(nn.Module):\n",
    "    def __init__(self, N, M, K, omega, I_max=120, J=10):\n",
    "        super(UPGANet, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            UPGANetLayer(N, M, K, omega, J=J) for _ in range(I_max)\n",
    "        ])\n",
    "        self.I_max = I_max\n",
    "        self.omega = omega\n",
    "\n",
    "    def forward(self, H, A0, D0, Psi, sigma_n2, P_BS):\n",
    "        A, D = A0, D0\n",
    "        for i in range(self.I_max):\n",
    "            A, D = self.layers[i](H, A, D, Psi, sigma_n2, P_BS)\n",
    "        return A, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "08075777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upganet_loss(H, A, D, Psi, sigma_n2, omega, theta_grid):\n",
    "    R = compute_rate(H, A, D, sigma_n2)\n",
    "    tau = compute_tau(A, D, Psi, theta_grid)\n",
    "    return -(R - omega * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "004f0fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training with J = 1 ===\n",
      "Channlel number 1/100 processed.\n",
      "Channlel number 2/100 processed.\n",
      "Channlel number 3/100 processed.\n",
      "Channlel number 4/100 processed.\n",
      "Channlel number 5/100 processed.\n",
      "Channlel number 6/100 processed.\n",
      "Channlel number 7/100 processed.\n",
      "Channlel number 8/100 processed.\n",
      "Channlel number 9/100 processed.\n",
      "Channlel number 10/100 processed.\n",
      "Channlel number 11/100 processed.\n",
      "Channlel number 12/100 processed.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[216]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m A_final, D_final = model(H_t, A0_t, D0_t, Psi_t, sigma_n2, P_BS_t)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Compute loss (negative objective)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m loss = \u001b[43mupganet_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPsi_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_n2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momega\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_grid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[32m     40\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[215]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mupganet_loss\u001b[39m\u001b[34m(H, A, D, Psi, sigma_n2, omega, theta_grid)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupganet_loss\u001b[39m(H, A, D, Psi, sigma_n2, omega, theta_grid):\n\u001b[32m      2\u001b[39m     R = compute_rate(H, A, D, sigma_n2)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     tau = \u001b[43mcompute_tau\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPsi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_grid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -(R - omega * tau)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[209]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mcompute_tau\u001b[39m\u001b[34m(A, D, Psi, theta_grid)\u001b[39m\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# convert a_theta to torch tensor\u001b[39;00m\n\u001b[32m     40\u001b[39m     a_theta = torch.tensor(a_theta, dtype=torch.cfloat)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     tau += torch.abs(a_theta.conj().T @ V @ V.conj().T @ a_theta - \u001b[43ma_theta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.T @ Psi @ a_theta)**\u001b[32m2\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tau / \u001b[38;5;28mlen\u001b[39m(theta_grid)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "J_values = [1, 10, 20]\n",
    "for J in J_values:\n",
    "    print(f\"=== Training with J = {J} ===\")\n",
    "\n",
    "    # Instantiate model\n",
    "    model = UPGANet(N, M, K, omega, I_max=I_max, J=J)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for _ in range(num_channels):\n",
    "            # 1. Generate random channel and Psi\n",
    "            H = generate_channel(N, M, L=3)\n",
    "            snr_values = np.arange(snr_min, snr_max, 0.1)\n",
    "            snr_db = np.random.choice(snr_values)\n",
    "            # P_BS = 10 ** (snr_db / 10)\n",
    "            \n",
    "            # FIX: unpack Psi and alpha_opt\n",
    "            Psi = compute_psi(snr_db)\n",
    "\n",
    "            A0, D0 = proposed_initialization(H, theta_d, N, M, K, P_BS)\n",
    "\n",
    "            # Convert to tensors\n",
    "            H_t = to_tensor(H, dtype=torch.cfloat)\n",
    "            Psi_t = to_tensor(Psi, dtype=torch.cfloat)\n",
    "            A0_t = to_tensor(A0, dtype=torch.cfloat)\n",
    "            D0_t = to_tensor(D0, dtype=torch.cfloat)\n",
    "\n",
    "            # sigma_n2 = to_tensor(sigma_n2, dtype=torch.float32)\n",
    "            # P_BS = to_tensor(P_BS, dtype=torch.float32)\n",
    "            P_BS_t = torch.tensor(P_BS, dtype=torch.cfloat) \n",
    "\n",
    "            # Forward pass\n",
    "            A_final, D_final = model(H_t, A0_t, D0_t, Psi_t, sigma_n2, P_BS_t)\n",
    "\n",
    "            # Compute loss (negative objective)\n",
    "            loss = upganet_loss(H_t, A_final, D_final, Psi_t, sigma_n2, omega, theta_grid)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            print(f\"Channlel number {_+1}/{num_channels} processed.\")\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] Loss: {total_loss/num_channels:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
