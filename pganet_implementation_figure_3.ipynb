{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e3a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "import uuid\n",
    "import cvxpy as cp\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "593a49b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define System and Simulation Parameters\n",
    "N = 64  # Number of BS antennas\n",
    "K = 4   # Number of users\n",
    "M = 4   # Number of RF chains\n",
    "omega = 0.3  # Tradeoff weight\n",
    "I_max = 10  # Maximum outer iterations\n",
    "J = 10  # Can be 1, 10, or 20\n",
    "\n",
    "SNR_dB = 12  # SNR in dB\n",
    "sigma_n2 = 1  # Noise variance\n",
    "P_BS = sigma_n2 * 10**(SNR_dB / 10)  # Transmit power\n",
    "mu = 0.01  # Step size for analog precoder\n",
    "lambda_ = 0.01  # Step size for digital precoder\n",
    "L = 20  # Number of paths for channel\n",
    "num_realizations = 2  # Number of channel realizations\n",
    "\n",
    "\n",
    "# Dataset parameters\n",
    "num_channels = 10\n",
    "num_epochs = 10 if J == 1 else 3\n",
    "snr_min, snr_max = 0, 12  # dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "867e246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define Sensing Parameters\n",
    "P = 3  # Number of desired sensing angles\n",
    "theta_d = np.array([-60, 0, 60]) * np.pi / 180  # Desired angles in radians\n",
    "delta_theta = 5 * np.pi / 180  # Half beamwidth\n",
    "theta_grid = np.linspace(-np.pi / 2, np.pi / 2, 181)  # Angular grid [-90, 90] degrees\n",
    "B_d = np.zeros(len(theta_grid))  # Desired beampattern\n",
    "for t, theta_t in enumerate(theta_grid):\n",
    "    for theta_p in theta_d:\n",
    "        if abs(theta_t - theta_p) <= delta_theta:\n",
    "            B_d[t] = 1\n",
    "\n",
    "# Wavenumber and antenna spacing\n",
    "lambda_wave = 1  # Wavelength (normalized)\n",
    "k = 2 * np.pi / lambda_wave\n",
    "d = lambda_wave / 2  # Antenna spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6ee6d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Step 3: Channel Matrix Generation (Saleh-Valenzuela Model)\n",
    "def generate_channel(N, M, L, device='cpu'):\n",
    "    H = torch.zeros((M, N), dtype=torch.cfloat, device=device)\n",
    "    for _ in range(L):\n",
    "        alpha = torch.randn(2, device=device).view(torch.cfloat)[0] / torch.sqrt(torch.tensor(2.0, device=device))\n",
    "        phi_r = torch.rand(1, device=device) * 2 * torch.pi\n",
    "        phi_t = torch.rand(1, device=device) * 2 * torch.pi\n",
    "        a_r = torch.exp(1j * k * d * torch.arange(M, dtype=torch.float32, device=device) * torch.sin(phi_r)) / torch.sqrt(torch.tensor(M, dtype=torch.float32, device=device))\n",
    "        a_t = torch.exp(1j * k * d * torch.arange(N, dtype=torch.float32, device=device) * torch.sin(phi_t)) / torch.sqrt(torch.tensor(N, dtype=torch.float32, device=device))\n",
    "        H += torch.sqrt(torch.tensor(N * M / L, dtype=torch.float32, device=device)) * alpha * torch.outer(a_r, a_t.conj())\n",
    "    return H\n",
    "\n",
    "# Steering vector function\n",
    "def steering_vector(theta, N):\n",
    "    return np.exp(1j * k * d * np.arange(N) * np.sin(theta)) / np.sqrt(N)\n",
    "\n",
    "# Compute benchmark covariance matrix Psi\n",
    "def compute_psi(N, Bd, theta_grid, PBS):\n",
    "    Abar_grid = np.exp(1j * np.pi * np.arange(N)[:, None] @ np.sin(theta_grid)[None, :])\n",
    "    T = 181;  \n",
    "    try:\n",
    "        import matlab.engine\n",
    "        eng = matlab.engine.start_matlab()\n",
    "\n",
    "        N_matlab = float(N)\n",
    "        T_matlab = float(T)\n",
    "        PBS_matlab = float(PBS)\n",
    "        Bd_matlab = matlab.double(Bd.tolist())\n",
    "        Abar_grid_matlab = matlab.double(Abar_grid.tolist(), is_complex=True)\n",
    "\n",
    "        # 3. Execute the MATLAB CVX function\n",
    "        print(\"Calling MATLAB CVX optimization...\")\n",
    "    \n",
    "        # Call the function and get the result\n",
    "        Psi_matlab_result = eng.solve_psi_cvx(\n",
    "            N_matlab, \n",
    "            T_matlab, \n",
    "            Bd_matlab, \n",
    "            Abar_grid_matlab, \n",
    "            PBS_matlab, \n",
    "            nargout=1 # Specify that the function returns 1 output\n",
    "        )\n",
    "    \n",
    "        # 4. Convert Result Back to NumPy\n",
    "        # .array() is often the most reliable way to convert MATLAB data back to NumPy\n",
    "        Psi_optimized_python = np.array(Psi_matlab_result, dtype=complex)\n",
    "    \n",
    "        print(\"Optimization successful. Resulting Psi matrix shape:\", Psi_optimized_python.shape)\n",
    "\n",
    "    except matlab.engine.EngineError as e:\n",
    "        print(f\"FATAL ERROR: Could not start or communicate with MATLAB Engine: {e}\")\n",
    "        # Handle the error by using the fallback here if needed\n",
    "        Psi_optimized_python = (PBS / N) * np.eye(N, dtype=complex)\n",
    "\n",
    "    finally:\n",
    "        # 5. Stop the MATLAB Engine (CRITICAL for resource cleanup)\n",
    "        if 'eng' in locals() and eng:\n",
    "            eng.quit()\n",
    "\n",
    "\n",
    "# Compute communication rate R\n",
    "def compute_rate(H, A, D, sigma_n2):\n",
    "    H_A = H @ A  # Effective channel\n",
    "    R = 0\n",
    "    for k in range(K):\n",
    "        h_k = H_A[:, k]\n",
    "        \n",
    "        signal = torch.abs(h_k.conj().T @ D[:, k])**2\n",
    "        interference = sum(torch.abs(h_k.conj().T @ D[:, j])**2 for j in range(K) if j != k)\n",
    "        SINR = signal / (interference + sigma_n2)\n",
    "        R += torch.log2(1 + SINR)\n",
    "    return R\n",
    "\n",
    "# Compute sensing error tau\n",
    "def compute_tau(A, D, Psi, theta_grid):\n",
    "    V = A @ D\n",
    "    tau = 0\n",
    "    for theta in theta_grid:\n",
    "        a_theta = steering_vector(theta, N)\n",
    "        # convert a_theta to torch tensor\n",
    "        a_theta = torch.tensor(a_theta, dtype=torch.cfloat)\n",
    "        tau += torch.abs(a_theta.conj().T @ V @ V.conj().T @ a_theta - a_theta.conj().T @ Psi @ a_theta)**2\n",
    "    return tau / len(theta_grid)\n",
    "\n",
    "def gradient_R_A(H, A, D, sigma_n2):\n",
    "    xi = 1 / torch.log(torch.tensor(2.0, dtype=A.dtype, device=A.device))\n",
    "    grad_A = torch.zeros_like(A, dtype=torch.cfloat)\n",
    "\n",
    "    # Effective covariance of the digital precoder\n",
    "    V = D @ D.conj().transpose(-2, -1)\n",
    "\n",
    "    for k in range(K):\n",
    "        # User-k effective channel outer product\n",
    "        h_k = H[k, :].reshape(-1, 1)  # (M x 1)\n",
    "        H_tilde_k = h_k @ h_k.conj().transpose(-2, -1)\n",
    "\n",
    "        # D_bar_k = D with user k's column set to zero\n",
    "        D_bar_k = D.clone()\n",
    "        D_bar_k[:, k] = 0.0\n",
    "        V_bar_k = D_bar_k @ D_bar_k.conj().transpose(-2, -1)\n",
    "\n",
    "        # Denominator terms (trace parts)\n",
    "        denom1 = torch.trace(A @ V @ A.conj().transpose(-2, -1) @ H_tilde_k) + sigma_n2\n",
    "        denom2 = torch.trace(A @ V_bar_k @ A.conj().transpose(-2, -1) @ H_tilde_k) + sigma_n2\n",
    "\n",
    "        # Gradient contribution\n",
    "        term1 = H_tilde_k @ A @ V / denom1\n",
    "        term2 = H_tilde_k @ A @ V_bar_k / denom2\n",
    "\n",
    "        grad_A += xi * (term1 - term2)\n",
    "\n",
    "    return grad_A\n",
    "\n",
    "def gradient_R_D(H, A, D, sigma_n2):\n",
    "    xi = 1 / torch.log(torch.tensor(2.0, dtype=A.dtype, device=A.device))\n",
    "    grad_D = torch.zeros_like(D, dtype=torch.cfloat)\n",
    "\n",
    "    for k in range(K):\n",
    "        # Channel vector for user k\n",
    "        h_k = H[k, :].reshape(-1, 1)  # (N x 1)\n",
    "        H_tilde_k = h_k @ h_k.conj().transpose(-2, -1)\n",
    "\n",
    "        # Effective digital-domain channel including analog precoder\n",
    "        H_bar_k = A.conj().transpose(-2, -1) @ H_tilde_k @ A\n",
    "\n",
    "        # D_bar_k = D with k-th column set to zero\n",
    "        D_bar_k = D.clone()\n",
    "        D_bar_k[:, k] = 0.0\n",
    "\n",
    "        # Compute denominator terms (trace parts)\n",
    "        denom1 = torch.trace(D @ D.conj().transpose(-2, -1) @ H_bar_k) + sigma_n2\n",
    "        denom2 = torch.trace(D_bar_k @ D_bar_k.conj().transpose(-2, -1) @ H_bar_k) + sigma_n2\n",
    "\n",
    "        # Compute gradient contributions\n",
    "        term1 = (H_bar_k @ D) / denom1\n",
    "        term2 = (H_bar_k @ D_bar_k) / denom2\n",
    "\n",
    "        grad_D += xi * (term1 - term2)\n",
    "\n",
    "    return grad_D\n",
    "\n",
    "def gradient_tau_A(A, D, Psi):\n",
    "    U = A @ D @ D.conj().transpose(-2, -1) @ A.conj().transpose(-2, -1)  # A D D^H A^H\n",
    "    grad_A = 2 * (U - Psi) @ A @ D @ D.conj().transpose(-2, -1)\n",
    "    return grad_A\n",
    "\n",
    "def gradient_tau_D(A, D, Psi):\n",
    "    U = A @ D @ D.conj().transpose(-2, -1) @ A.conj().transpose(-2, -1)  # A D D^H A^H\n",
    "    grad_D = 2 * A.conj().transpose(-2, -1) @ (U - Psi) @ A @ D\n",
    "    return grad_D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a289331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "def proposed_initialization(H, theta_d, N, M, K, P_BS):\n",
    "\n",
    "    theta_d = torch.tensor(theta_d, dtype=torch.float32, device=H.device) if isinstance(theta_d, np.ndarray) else theta_d\n",
    "    steering_des = torch.exp(1j * torch.pi * torch.arange(N, dtype=torch.float32, device=H.device).reshape(-1, 1) * torch.sin(theta_d[:M-K].reshape(1, -1)))\n",
    "    G = torch.cat((H.T, steering_des), dim=1)  # shape (N, K + (M-K)) = (64, 4) if M=K\n",
    "    A0 = torch.exp(-1j * torch.angle(G[:, :M]))  # shape (N, M) = (64, 4)\n",
    "    \n",
    "    X_ZF = torch.linalg.pinv(H)  # shape (N, K) = (64, 4)\n",
    "    D0 = torch.linalg.pinv(A0) @ X_ZF  # shape (M, K) = (4, 4)\n",
    "    norm_factor = torch.norm(A0 @ D0, p='fro')\n",
    "    D0 = torch.sqrt(torch.tensor(P_BS, dtype=A0.dtype, device=A0.device)) * D0 / norm_factor\n",
    "\n",
    "    \n",
    "    return A0, D0\n",
    "\n",
    "def random_initialization(N, M, H, P_BS):\n",
    "    A0 = np.exp(1j * np.random.uniform(0, 2 * np.pi, (N, M)))\n",
    "    D0 = np.linalg.pinv(H @ A0)\n",
    "    D0 = np.sqrt(P_BS) * D0 / np.linalg.norm(A0 @ D0, 'fro')\n",
    "    return A0, D0\n",
    "\n",
    "def svd_initialization(H, N, M, K, P_BS):\n",
    "    _, _, Vh = svd(H, full_matrices=False)  # Vh is N x M (conjugate of right singular vectors)\n",
    "    A0 = Vh.T[:, :M]  # Take first M columns, shape N x M\n",
    "    A0 = np.exp(1j * np.angle(A0))  # Project to unit modulus\n",
    "    H_A = H @ A0  # Shape: M x M\n",
    "    try:\n",
    "        D0 = np.linalg.pinv(H_A)  # Pseudoinverse of H @ A0\n",
    "    except np.linalg.LinAlgError:\n",
    "        D0 = np.linalg.pinv(H_A + 1e-6 * np.eye(M))  # Regularization for stability\n",
    "    D0 = np.sqrt(P_BS) * D0 / np.linalg.norm(A0 @ D0, 'fro')  # Normalize\n",
    "    return A0, D0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f9f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_unit_modulus(A):\n",
    "    return torch.exp(1j * torch.angle(A))\n",
    "\n",
    "def project_power_constraint(A, D, P_BS):\n",
    "    norm_factor = torch.norm(A @ D, p='fro')\n",
    "    D = D * (torch.sqrt(torch.tensor(P_BS, dtype=D.dtype, device=D.device)) / norm_factor)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f2be924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pga(H, A0, D0, J, I_max, mu, lambda_, omega, sigma_n2, Psi, theta_grid):\n",
    "    N, K = H.shape\n",
    "    A = A0.copy()\n",
    "    D = D0.copy()\n",
    "    objectives = []\n",
    "    eta = 1 / N  # Balancing term for gradient magnitudes\n",
    "\n",
    "    for i in range(I_max):\n",
    "        print(f\"\\n===== Outer Iteration {i+1}/{I_max} =====\")\n",
    "\n",
    "        # ---- Inner Loop: Analog Precoder Update ----\n",
    "        A_hat = A.copy()\n",
    "        for j in range(J):\n",
    "            grad_R_A = gradient_R_A(H, A_hat, D, sigma_n2)\n",
    "            grad_tau_A = gradient_tau_A(A_hat, D, Psi)\n",
    "\n",
    "            # Eq. (14b): Gradient Ascent on A\n",
    "            grad_A = grad_R_A - omega * grad_tau_A\n",
    "            A_hat = A_hat + mu * grad_A\n",
    "\n",
    "            # Eq. (7): Unit Modulus Projection\n",
    "            A_hat = project_unit_modulus(A_hat)\n",
    "\n",
    "        A = A_hat.copy()  # Set final A after J inner updates\n",
    "\n",
    "        # ---- Outer Loop: Digital Precoder Update ----\n",
    "        grad_R_D = gradient_R_D(H, A, D, sigma_n2)\n",
    "        grad_tau_D = gradient_tau_D(A, D, Psi)\n",
    "\n",
    "        # Eq. (15): Gradient Ascent on D\n",
    "        grad_D = grad_R_D - omega * eta * grad_tau_D\n",
    "        D = D + lambda_ * grad_D\n",
    "\n",
    "        # Eq. (9): Power Constraint Projection\n",
    "        D = project_power_constraint(A,D, P_BS)\n",
    "\n",
    "        # ---- Compute Objective (Eq. 5a) ----\n",
    "        R = compute_rate(H, A, D, sigma_n2)\n",
    "        tau = compute_tau(A, D, Psi, theta_grid)\n",
    "        objective = R - omega * tau\n",
    "        objectives.append(objective)\n",
    "\n",
    "        print(f\"Iteration {i+1}: R = {R:.4f}, τ = {tau:.4e}, Objective = {objective:.4f}\")\n",
    "\n",
    "    return objectives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d6cf8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UPGANetLayer(nn.Module):\n",
    "    def __init__(self, N, M, K, omega, J=10, eta=None):\n",
    "        super(UPGANetLayer, self).__init__()\n",
    "        self.J = J\n",
    "        self.N, self.M, self.K = N, M, K\n",
    "        self.omega = omega\n",
    "        self.eta = eta if eta is not None else 1/N\n",
    "\n",
    "        # Learnable step sizes (initialized to μ(0,0)=λ(0)=0.01)\n",
    "        self.mu = nn.Parameter(torch.full((J,), 0.01, dtype=torch.float32))\n",
    "        self.lambda_ = nn.Parameter(torch.tensor(0.01, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, H, A, D, Psi, sigma_n2, P_BS):\n",
    "        # --- J inner updates for analog precoder ---\n",
    "        for j in range(self.J):\n",
    "            grad_RA = gradient_R_A(H, A, D, sigma_n2)\n",
    "            grad_tauA = gradient_tau_A(A, D, Psi)\n",
    "            A = A + self.mu[j] * (grad_RA - self.omega * grad_tauA)\n",
    "            A = project_unit_modulus(A)\n",
    "\n",
    "        # --- Digital precoder update ---\n",
    "        grad_RD = gradient_R_D(H, A, D, sigma_n2)\n",
    "        grad_tauD = gradient_tau_D(A, D, Psi)\n",
    "        D = D + self.lambda_ * (grad_RD - self.omega * self.eta * grad_tauD)\n",
    "        D = project_power_constraint(A, D, P_BS)\n",
    "\n",
    "        return A, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "858143a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UPGANet(nn.Module):\n",
    "    def __init__(self, N, M, K, omega, I_max=120, J=10):\n",
    "        super(UPGANet, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            UPGANetLayer(N, M, K, omega, J=J) for _ in range(I_max)\n",
    "        ])\n",
    "        self.I_max = I_max\n",
    "        self.omega = omega\n",
    "\n",
    "    def forward(self, H, A0, D0, Psi, sigma_n2, P_BS):\n",
    "        A, D = A0, D0\n",
    "        for i in range(self.I_max):\n",
    "            A, D = self.layers[i](H, A, D, Psi, sigma_n2, P_BS)\n",
    "        return A, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08075777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upganet_loss(H, A, D, Psi, sigma_n2, omega, theta_grid):\n",
    "    R = compute_rate(H, A, D, sigma_n2)\n",
    "    tau = compute_tau(A, D, Psi, theta_grid)\n",
    "    return -(R - omega * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "004f0fc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'matlab' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mcompute_psi\u001b[39m\u001b[34m(N, Bd, theta_grid, PBS)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatlab\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\n\u001b[32m     25\u001b[39m     eng = matlab.engine.start_matlab()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matlab'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m P_BS = \u001b[32m10\u001b[39m ** (snr_db / \u001b[32m10\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# FIX: unpack Psi and alpha_opt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m Psi, alpha_opt = \u001b[43mcompute_psi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP_BS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m A0, D0 = proposed_initialization(H, theta_d, N, M, K, P_BS)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Ensure Psi is a proper NumPy array\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mcompute_psi\u001b[39m\u001b[34m(N, Bd, theta_grid, PBS)\u001b[39m\n\u001b[32m     48\u001b[39m     Psi_optimized_python = np.array(Psi_matlab_result, dtype=\u001b[38;5;28mcomplex\u001b[39m)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOptimization successful. Resulting Psi matrix shape:\u001b[39m\u001b[33m\"\u001b[39m, Psi_optimized_python.shape)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mmatlab\u001b[49m.engine.EngineError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     53\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFATAL ERROR: Could not start or communicate with MATLAB Engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m     \u001b[38;5;66;03m# Handle the error by using the fallback here if needed\u001b[39;00m\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'matlab' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = UPGANet(N, M, K, omega, I_max=I_max, J=J)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for _ in range(num_channels):\n",
    "        # 1. Generate random channel and Psi\n",
    "        H = generate_channel(N, M, L=3)\n",
    "        snr_db = np.random.uniform(snr_min, snr_max)\n",
    "        P_BS = 10 ** (snr_db / 10)\n",
    "        \n",
    "        # FIX: unpack Psi and alpha_opt\n",
    "        Psi, alpha_opt = compute_psi(N, B_d, theta_grid, P_BS)\n",
    "\n",
    "        A0, D0 = proposed_initialization(H, theta_d, N, M, K, P_BS)\n",
    "\n",
    "        # Ensure Psi is a proper NumPy array\n",
    "        Psi = np.array(Psi, dtype=np.complex64)\n",
    "        H = np.array(H, dtype=np.complex64)\n",
    "        A0 = np.array(A0, dtype=np.complex64)\n",
    "        D0 = np.array(D0, dtype=np.complex64)\n",
    "\n",
    "        # Convert to tensors\n",
    "        H_t = torch.tensor(H, dtype=torch.cfloat)\n",
    "        Psi_t = torch.tensor(Psi, dtype=torch.cfloat)\n",
    "        A0_t = torch.tensor(A0, dtype=torch.cfloat)\n",
    "        D0_t = torch.tensor(D0, dtype=torch.cfloat)\n",
    "\n",
    "        # Forward pass\n",
    "        A_final, D_final = model(H_t, A0_t, D0_t, Psi_t, sigma_n2, P_BS)\n",
    "\n",
    "        # Compute loss (negative objective)\n",
    "        loss = upganet_loss(H_t, A_final, D_final, Psi_t, sigma_n2, omega, theta_grid)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{num_epochs}] Loss: {total_loss/num_channels:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
