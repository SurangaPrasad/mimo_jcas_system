{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "45e3a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "import uuid\n",
    "import cvxpy as cp\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from scipy.io import loadmat\n",
    "import h5py\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "593a49b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define System and Simulation Parameters\n",
    "N = 64  # Number of BS antennas\n",
    "K = 4   # Number of users\n",
    "M = 4   # Number of RF chains\n",
    "omega = 0.3  # Tradeoff weight\n",
    "I_max = 120  # Maximum outer iterations\n",
    "J = 10  # Can be 1, 10, or 20\n",
    "\n",
    "SNR_dB = 12  # SNR in dB\n",
    "sigma_n2 = 1  # Noise variance\n",
    "P_BS = sigma_n2 * 10**(SNR_dB / 10)  # Transmit power\n",
    "mu = 0.01  # Step size for analog precoder\n",
    "lambda_ = 0.01  # Step size for digital precoder\n",
    "L = 20  # Number of paths for channel\n",
    "num_realizations = 100  # Number of channel realizations\n",
    "\n",
    "\n",
    "# Dataset parameters\n",
    "num_channels = 100\n",
    "num_epochs = 100 if J == 100 else 30\n",
    "snr_min, snr_max = 0, 12  # dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "1005b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x, dtype=None, device=device):\n",
    "    \"\"\"Convert numpy or tensor input to a torch tensor on the right device.\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.to(device=device, dtype=dtype)\n",
    "    elif isinstance(x, np.ndarray):\n",
    "        return torch.as_tensor(x, dtype=dtype, device=device)\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported type: {type(x)}\")\n",
    "\n",
    "def to_numpy(x):\n",
    "    \"\"\"Convert tensor to numpy array (CPU)\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().numpy()\n",
    "    elif isinstance(x, np.ndarray):\n",
    "        return x\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported type: {type(x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "867e246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define Sensing Parameters\n",
    "P = 3  # Number of desired sensing angles\n",
    "theta_d = np.array([-60, 0, 60]) * np.pi / 180  # Desired angles in radians\n",
    "delta_theta = 5 * np.pi / 180  # Half beamwidth\n",
    "theta_grid = np.linspace(-np.pi / 2, np.pi / 2, 181)  # Angular grid [-90, 90] degrees\n",
    "B_d = np.zeros(len(theta_grid))  # Desired beampattern\n",
    "for t, theta_t in enumerate(theta_grid):\n",
    "    for theta_p in theta_d:\n",
    "        if abs(theta_t - theta_p) <= delta_theta:\n",
    "            B_d[t] = 1\n",
    "\n",
    "# Wavenumber and antenna spacing\n",
    "lambda_wave = 1  # Wavelength (normalized)\n",
    "k = 2 * np.pi / lambda_wave\n",
    "d = lambda_wave / 2  # Antenna spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "d6ee6d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Step 3: Channel Matrix Generation (Saleh-Valenzuela Model)\n",
    "def generate_channel_batch(N, M, L, batch_size=1, device=device):\n",
    "    \"\"\"Generate batch of channels directly on GPU\"\"\"\n",
    "    H = torch.zeros((batch_size, M, N), dtype=torch.cfloat, device=device)\n",
    "    \n",
    "    for _ in range(L):\n",
    "        # Generate complex gains\n",
    "        alpha_real = torch.randn(batch_size, device=device) / np.sqrt(2)\n",
    "        alpha_imag = torch.randn(batch_size, device=device) / np.sqrt(2)\n",
    "        alpha = torch.complex(alpha_real, alpha_imag).reshape(batch_size, 1, 1)\n",
    "        \n",
    "        # Random angles\n",
    "        phi_r = torch.rand(batch_size, device=device) * 2 * np.pi\n",
    "        phi_t = torch.rand(batch_size, device=device) * 2 * np.pi\n",
    "        \n",
    "        # Compute steering vectors\n",
    "        n_r = torch.arange(M, device=device).float()\n",
    "        n_t = torch.arange(N, device=device).float()\n",
    "        \n",
    "        # Broadcasting for batch computation\n",
    "        a_r = torch.exp(1j * k * d * n_r.view(1, -1, 1) * torch.sin(phi_r).view(-1, 1, 1)) / np.sqrt(M)\n",
    "        a_t = torch.exp(1j * k * d * n_t.view(1, 1, -1) * torch.sin(phi_t).view(-1, 1, 1)) / np.sqrt(N)\n",
    "        \n",
    "        # Outer product: a_r @ a_t.conj()\n",
    "        H += np.sqrt(N * M / L) * alpha * torch.bmm(a_r, a_t.conj())\n",
    "    \n",
    "    return H.squeeze(0) if batch_size == 1 else H\n",
    "\n",
    "# Steering vector function\n",
    "def generate_channel(N, M, L):\n",
    "    \"\"\"Single channel generation (for compatibility)\"\"\"\n",
    "    return generate_channel_batch(N, M, L, batch_size=1, device=device)\n",
    "\n",
    "# Steering vector function - optimized for GPU\n",
    "def steering_vector_batch(theta, N, device=device):\n",
    "    \"\"\"Compute steering vectors for batch of angles on GPU\"\"\"\n",
    "    n = torch.arange(N, device=device).float()\n",
    "    if theta.dim() == 0:  # scalar\n",
    "        theta = theta.unsqueeze(0)\n",
    "    # theta: (num_angles,), n: (N,)\n",
    "    return torch.exp(1j * k * d * n.unsqueeze(0) * torch.sin(theta).unsqueeze(1)) / np.sqrt(N)\n",
    "\n",
    "def steering_vector(theta, N):\n",
    "    \"\"\"Single steering vector\"\"\"\n",
    "    theta_t = to_tensor(theta, dtype=torch.float32)\n",
    "    return steering_vector_batch(theta_t, N).squeeze(0)\n",
    "\n",
    "\n",
    "# Compute communication rate R - optimized\n",
    "def compute_rate(H, A, D, sigma_n2):\n",
    "    \"\"\"Vectorized rate computation\"\"\"\n",
    "    H_A = H @ A  # (K x M) @ (M x N) -> (K x N) or batch\n",
    "    \n",
    "    # Handle both single and batch inputs\n",
    "    if H_A.dim() == 2:\n",
    "        H_A = H_A.unsqueeze(0)\n",
    "        D_expanded = D.unsqueeze(0)\n",
    "        batch_mode = False\n",
    "    else:\n",
    "        D_expanded = D\n",
    "        batch_mode = True\n",
    "    \n",
    "    batch_size = H_A.shape[0]\n",
    "    \n",
    "    # Compute all signals and interference at once\n",
    "    # H_A: (batch, K, N), D: (batch, N, K) or (N, K)\n",
    "    hk_dk = torch.einsum('bkn,bnk->bk', H_A.conj(), D_expanded)  # (batch, K)\n",
    "    signal = torch.abs(hk_dk) ** 2  # (batch, K)\n",
    "    \n",
    "    # Total power per user\n",
    "    hk_d = torch.einsum('bkn,bnj->bkj', H_A.conj(), D_expanded)  # (batch, K, K)\n",
    "    total_power = torch.abs(hk_d) ** 2  # (batch, K, K)\n",
    "    \n",
    "    # Interference = total - signal\n",
    "    interference = total_power.sum(dim=-1) - signal  # (batch, K)\n",
    "    \n",
    "    # SINR and rate\n",
    "    SINR = signal / (interference + sigma_n2)\n",
    "    R = torch.log2(1 + SINR).sum(dim=-1)  # (batch,)\n",
    "    \n",
    "    return R.squeeze() if not batch_mode else R\n",
    "\n",
    "# Compute sensing error tau\n",
    "def compute_tau(A, D, Psi, theta_grid_gpu):\n",
    "    \"\"\"Vectorized tau computation\"\"\"\n",
    "    V = A @ D\n",
    "    \n",
    "    # Compute steering vectors for all angles at once\n",
    "    a_theta = steering_vector_batch(theta_grid_gpu, A.shape[0])  # (num_angles, N)\n",
    "    \n",
    "    # Vectorized computation: a^H V V^H a - a^H Psi a\n",
    "    VVH = V @ V.conj().T  # (N, N)\n",
    "    \n",
    "    # Batch matrix-vector products\n",
    "    aH_VVH_a = torch.einsum('an,nm,am->a', a_theta.conj(), VVH, a_theta)\n",
    "    aH_Psi_a = torch.einsum('an,nm,am->a', a_theta.conj(), Psi, a_theta)\n",
    "    \n",
    "    tau = torch.abs(aH_VVH_a - aH_Psi_a) ** 2\n",
    "    return tau.mean()\n",
    "\n",
    "def gradient_R_A(H, A, D, sigma_n2):\n",
    "    \"\"\"Optimized gradient computation for A\"\"\"\n",
    "    xi = 1 / torch.log(torch.tensor(2.0, dtype=A.real.dtype, device=A.device))\n",
    "    grad_A = torch.zeros_like(A)\n",
    "    \n",
    "    # Precompute V and V_full\n",
    "    V = D @ D.conj().T  # (N, N)\n",
    "    \n",
    "    for k in range(K):\n",
    "        h_k = H[k, :].reshape(-1, 1)  # (N, 1)\n",
    "        H_tilde_k = h_k @ h_k.conj().T  # (N, N)\n",
    "        \n",
    "        # Compute V_bar_k more efficiently\n",
    "        d_k = D[:, k].reshape(-1, 1)\n",
    "        V_bar_k = V - d_k @ d_k.conj().T\n",
    "        \n",
    "        # Denominator terms\n",
    "        AV = A @ V\n",
    "        AVbar = A @ V_bar_k\n",
    "        denom1 = torch.trace(AV @ A.conj().T @ H_tilde_k) + sigma_n2\n",
    "        denom2 = torch.trace(AVbar @ A.conj().T @ H_tilde_k) + sigma_n2\n",
    "        \n",
    "        # Gradient contribution\n",
    "        term1 = H_tilde_k @ AV / denom1\n",
    "        term2 = H_tilde_k @ AVbar / denom2\n",
    "        \n",
    "        grad_A += xi * (term1 - term2)\n",
    "    \n",
    "    return grad_A\n",
    "\n",
    "def gradient_R_D(H, A, D, sigma_n2):\n",
    "    \"\"\"Optimized gradient computation for D\"\"\"\n",
    "    xi = 1 / torch.log(torch.tensor(2.0, dtype=A.real.dtype, device=A.device))\n",
    "    grad_D = torch.zeros_like(D)\n",
    "    \n",
    "    # Precompute A^H\n",
    "    AH = A.conj().T\n",
    "    \n",
    "    for k in range(K):\n",
    "        h_k = H[k, :].reshape(-1, 1)\n",
    "        H_tilde_k = h_k @ h_k.conj().T\n",
    "        H_bar_k = AH @ H_tilde_k @ A  # (M, M)\n",
    "        \n",
    "        # Compute D_bar_k more efficiently\n",
    "        d_k = D[:, k].reshape(-1, 1)\n",
    "        DDH = D @ D.conj().T\n",
    "        DDH_bar = DDH - d_k @ d_k.conj().T\n",
    "        \n",
    "        # Denominator terms\n",
    "        denom1 = torch.trace(DDH @ H_bar_k) + sigma_n2\n",
    "        denom2 = torch.trace(DDH_bar @ H_bar_k) + sigma_n2\n",
    "        \n",
    "        # Gradient contributions\n",
    "        term1 = (H_bar_k @ D) / denom1\n",
    "        term2 = (H_bar_k @ (D - d_k @ torch.eye(K, device=D.device)[k].reshape(1, -1))) / denom2\n",
    "        \n",
    "        grad_D += xi * (term1 - term2)\n",
    "    \n",
    "    return grad_D\n",
    "\n",
    "def gradient_tau_A(A, D, Psi):\n",
    "    \"\"\"Optimized gradient for tau w.r.t. A\"\"\"\n",
    "    DDH = D @ D.conj().T\n",
    "    U = A @ DDH @ A.conj().T\n",
    "    grad_A = 2 * (U - Psi) @ A @ DDH\n",
    "    return grad_A\n",
    "\n",
    "def gradient_tau_D(A, D, Psi):\n",
    "    \"\"\"Optimized gradient for tau w.r.t. D\"\"\"\n",
    "    AHA = A.conj().T @ A\n",
    "    U = A @ D @ D.conj().T @ A.conj().T\n",
    "    grad_D = 2 * A.conj().T @ (U - Psi) @ A @ D\n",
    "    return grad_D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd9199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Psi_all', 'SNR_dB']\n",
      "[('real', '<f8'), ('imag', '<f8')]\n",
      "Psi_all shape: (121, 64, 64)\n",
      "SNR_dB: [ 0.   0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.   1.1  1.2  1.3\n",
      "  1.4  1.5  1.6  1.7  1.8  1.9  2.   2.1  2.2  2.3  2.4  2.5  2.6  2.7\n",
      "  2.8  2.9  3.   3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9  4.   4.1\n",
      "  4.2  4.3  4.4  4.5  4.6  4.7  4.8  4.9  5.   5.1  5.2  5.3  5.4  5.5\n",
      "  5.6  5.7  5.8  5.9  6.   6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9\n",
      "  7.   7.1  7.2  7.3  7.4  7.5  7.6  7.7  7.8  7.9  8.   8.1  8.2  8.3\n",
      "  8.4  8.5  8.6  8.7  8.8  8.9  9.   9.1  9.2  9.3  9.4  9.5  9.6  9.7\n",
      "  9.8  9.9 10.  10.1 10.2 10.3 10.4 10.5 10.6 10.7 10.8 10.9 11.  11.1\n",
      " 11.2 11.3 11.4 11.5 11.6 11.7 11.8 11.9 12. ]\n",
      "(64, 64)\n",
      "[[0.15625    0.01752379 0.05855906 ... 0.11866832 0.13690259 0.0114427 ]\n",
      " [0.01752379 0.15625    0.08262141 ... 0.03946006 0.07101367 0.13690259]\n",
      " [0.05855906 0.08262141 0.15625    ... 0.13355056 0.03946006 0.11866832]\n",
      " ...\n",
      " [0.11866832 0.03946006 0.13355056 ... 0.15625    0.08262141 0.05855906]\n",
      " [0.13690259 0.07101367 0.03946006 ... 0.08262141 0.15625    0.01752379]\n",
      " [0.0114427  0.13690259 0.11866832 ... 0.05855906 0.01752379 0.15625   ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with h5py.File('Psi_all.mat', 'r') as f:\n",
    "\n",
    "    # Read MATLAB complex dataset properly\n",
    "    Psi_h5 = f['Psi_all']\n",
    "\n",
    "    # If it's a compound dtype (MATLAB complex), split real/imag\n",
    "    if np.issubdtype(Psi_h5.dtype, np.void):\n",
    "        real = Psi_h5['real'][()]  # convert to numpy array\n",
    "        imag = Psi_h5['imag'][()]\n",
    "        Psi_all = real + 1j*imag\n",
    "    else:\n",
    "        Psi_all = np.array(Psi_h5)\n",
    "\n",
    "    SNR_dB = np.array(f['SNR_dB']).flatten()\n",
    "\n",
    "\n",
    "# Access Psi\n",
    "def compute_psi(snr_db):\n",
    "    idx = np.argmin(np.abs(SNR_dB - snr_db))\n",
    "    return Psi_all[idx, :, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "92f9f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_unit_modulus(A):\n",
    "    \"\"\"Fast unit modulus projection\"\"\"\n",
    "    return torch.exp(1j * torch.angle(A))\n",
    "\n",
    "def project_power_constraint(A, D, P_BS):\n",
    "    \"\"\"Fast power constraint projection\"\"\"\n",
    "    norm_factor = torch.linalg.norm(A @ D, ord='fro')\n",
    "    D = D * (torch.sqrt(P_BS) / norm_factor)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "d0679ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proposed_initialization(H, theta_d, N, M, K, P_BS):\n",
    "    \"\"\"Initialization - done on GPU\"\"\"\n",
    "    # Convert to numpy for pinv operations (more stable)\n",
    "    H_np = to_numpy(H)\n",
    "    G = np.array([H_np[k, :] for k in range(K)]).T\n",
    "    A0 = np.exp(1j * np.angle(G))\n",
    "    X_ZF = np.linalg.pinv(H_np)\n",
    "    D0 = np.linalg.pinv(A0) @ X_ZF\n",
    "    D0 = np.sqrt(P_BS) * D0 / np.linalg.norm(A0 @ D0, 'fro')\n",
    "    \n",
    "    # Move back to GPU\n",
    "    return to_tensor(A0, dtype=torch.cfloat), to_tensor(D0, dtype=torch.cfloat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "8d6cf8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UPGANetLayer(nn.Module):\n",
    "    def __init__(self, N, M, K, omega, J=10, eta=None):\n",
    "        super(UPGANetLayer, self).__init__()\n",
    "        self.J = J\n",
    "        self.N, self.M, self.K = N, M, K\n",
    "        self.omega = omega\n",
    "        self.eta = eta if eta is not None else 1/N\n",
    "        \n",
    "        # Learnable step sizes\n",
    "        self.mu = nn.Parameter(torch.full((J,), 0.01, dtype=torch.float32))\n",
    "        self.lambda_ = nn.Parameter(torch.tensor(0.01, dtype=torch.float32))\n",
    "    \n",
    "    def forward(self, H, A, D, Psi, sigma_n2, P_BS):\n",
    "        # J inner updates for analog precoder\n",
    "        for j in range(self.J):\n",
    "            grad_RA = gradient_R_A(H, A, D, sigma_n2)\n",
    "            grad_tauA = gradient_tau_A(A, D, Psi)\n",
    "            A = A + self.mu[j] * (grad_RA - self.omega * grad_tauA)\n",
    "            A = project_unit_modulus(A)\n",
    "        \n",
    "        # Digital precoder update\n",
    "        grad_RD = gradient_R_D(H, A, D, sigma_n2)\n",
    "        grad_tauD = gradient_tau_D(A, D, Psi)\n",
    "        D = D + self.lambda_ * (grad_RD - self.omega * self.eta * grad_tauD)\n",
    "        D = project_power_constraint(A, D, P_BS)\n",
    "        \n",
    "        return A, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "858143a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UPGANet(nn.Module):\n",
    "    def __init__(self, N, M, K, omega, I_max=120, J=10):\n",
    "        super(UPGANet, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            UPGANetLayer(N, M, K, omega, J=J) for _ in range(I_max)\n",
    "        ])\n",
    "        self.I_max = I_max\n",
    "        self.omega = omega\n",
    "    \n",
    "    def forward(self, H, A0, D0, Psi, sigma_n2, P_BS):\n",
    "        A, D = A0, D0\n",
    "        for i in range(self.I_max):\n",
    "            A, D = self.layers[i](H, A, D, Psi, sigma_n2, P_BS)\n",
    "        return A, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "08075777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upganet_loss(H, A, D, Psi, sigma_n2, omega, theta_grid_gpu):\n",
    "    \"\"\"Loss computation using GPU theta_grid\"\"\"\n",
    "    R = compute_rate(H, A, D, sigma_n2)\n",
    "    tau = compute_tau(A, D, Psi, theta_grid_gpu)\n",
    "    return -(R - omega * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "004f0fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training with J = 1 ===\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected m1 and m2 to have the same dtype, but got: c10::complex<float> != float",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[520]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m P_BS_t = torch.tensor(P_BS, dtype=torch.float32, device=device)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m A_final, D_final = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPsi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_n2_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP_BS_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[32m     37\u001b[39m loss = upganet_loss(H, A_final, D_final, Psi, sigma_n2_t, omega, theta_grid_gpu)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/OneDrive/PHD Thesis/pganet_implementation/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/OneDrive/PHD Thesis/pganet_implementation/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[518]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mUPGANet.forward\u001b[39m\u001b[34m(self, H, A0, D0, Psi, sigma_n2, P_BS)\u001b[39m\n\u001b[32m     11\u001b[39m A, D = A0, D0\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.I_max):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     A, D = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPsi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_n2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP_BS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m A, D\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/OneDrive/PHD Thesis/pganet_implementation/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/OneDrive/PHD Thesis/pganet_implementation/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[517]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mUPGANetLayer.forward\u001b[39m\u001b[34m(self, H, A, D, Psi, sigma_n2, P_BS)\u001b[39m\n\u001b[32m     19\u001b[39m     A = project_unit_modulus(A)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Digital precoder update\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m grad_RD = \u001b[43mgradient_R_D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_n2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m grad_tauD = gradient_tau_D(A, D, Psi)\n\u001b[32m     24\u001b[39m D = D + \u001b[38;5;28mself\u001b[39m.lambda_ * (grad_RD - \u001b[38;5;28mself\u001b[39m.omega * \u001b[38;5;28mself\u001b[39m.eta * grad_tauD)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[513]\u001b[39m\u001b[32m, line 157\u001b[39m, in \u001b[36mgradient_R_D\u001b[39m\u001b[34m(H, A, D, sigma_n2)\u001b[39m\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# Gradient contributions\u001b[39;00m\n\u001b[32m    156\u001b[39m     term1 = (H_bar_k @ D) / denom1\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     term2 = (H_bar_k @ (D - \u001b[43md_k\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mD\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m)) / denom2\n\u001b[32m    159\u001b[39m     grad_D += xi * (term1 - term2)\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m grad_D\n",
      "\u001b[31mRuntimeError\u001b[39m: expected m1 and m2 to have the same dtype, but got: c10::complex<float> != float"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "J_values = [1, 10, 20]\n",
    "for J in J_values:\n",
    "    print(f\"\\n=== Training with J = {J} ===\")\n",
    "    \n",
    "    # Move model to GPU\n",
    "    model = UPGANet(N, M, K, omega, I_max=I_max, J=J).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Pre-generate SNR values on GPU\n",
    "    snr_values = torch.arange(snr_min, snr_max, 0.1, device=device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for ch_idx in range(num_channels):\n",
    "            # Generate channel on GPU\n",
    "            H = generate_channel(N, M, L=3)\n",
    "            \n",
    "            # Random SNR\n",
    "            snr_db = snr_values[torch.randint(len(snr_values), (1,), device=device)].item()\n",
    "            \n",
    "            # Get Psi on GPU\n",
    "            Psi = compute_psi(snr_db)\n",
    "            \n",
    "            # Initialization on GPU\n",
    "            A0, D0 = proposed_initialization(H, theta_d, N, M, K, P_BS)\n",
    "            \n",
    "            # Convert scalars to tensors on GPU\n",
    "            sigma_n2_t = torch.tensor(sigma_n2, dtype=torch.float32, device=device)\n",
    "            P_BS_t = torch.tensor(P_BS, dtype=torch.float32, device=device)\n",
    "            \n",
    "            # Forward pass\n",
    "            A_final, D_final = model(H, A0, D0, Psi, sigma_n2_t, P_BS_t)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = upganet_loss(H, A_final, D_final, Psi, sigma_n2_t, omega, theta_grid_gpu)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if (ch_idx + 1) % 10 == 0:\n",
    "                print(f\"Channel {ch_idx+1}/{num_channels} processed.\")\n",
    "        \n",
    "        avg_loss = total_loss / num_channels\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] Average Loss: {avg_loss:.6f}\")\n",
    "        \n",
    "        # Clear CUDA cache periodically\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
