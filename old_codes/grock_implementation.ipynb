{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4d915df",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 308\u001b[39m\n\u001b[32m    306\u001b[39m A0_t = A0.to(device)\n\u001b[32m    307\u001b[39m D0_t = D0.to(device)\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m A_final, D_final = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA0_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD0_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPsi_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_n2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP_BS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m loss = upganet_loss(H_t, A_final, D_final, Psi_t, sigma_n2, omega)\n\u001b[32m    310\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/OneDrive/PHD Thesis/pganet_implementation/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/OneDrive/PHD Thesis/pganet_implementation/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 277\u001b[39m, in \u001b[36mUPGANet.forward\u001b[39m\u001b[34m(self, H, A0, D0, Psi, sigma_n2, P_BS)\u001b[39m\n\u001b[32m    275\u001b[39m A, D = A0, D0\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.I_max):\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     A, D = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPsi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_n2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP_BS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m A, D\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/OneDrive/PHD Thesis/pganet_implementation/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/OneDrive/PHD Thesis/pganet_implementation/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 256\u001b[39m, in \u001b[36mUPGANetLayer.forward\u001b[39m\u001b[34m(self, H, A, D, Psi, sigma_n2, P_BS)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.J):\n\u001b[32m    255\u001b[39m     grad_RA = gradient_R_A(H, A, D, sigma_n2)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     grad_tauA = \u001b[43mgradient_tau_A\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPsi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m     A = A + \u001b[38;5;28mself\u001b[39m.mu[j] * (grad_RA - \u001b[38;5;28mself\u001b[39m.omega * grad_tauA)\n\u001b[32m    258\u001b[39m     A = project_unit_modulus(A)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 165\u001b[39m, in \u001b[36mgradient_tau_A\u001b[39m\u001b[34m(A, D, Psi)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgradient_tau_A\u001b[39m(A, D, Psi):\n\u001b[32m    164\u001b[39m     U = A @ D @ D.conj().transpose(-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m) @ A.conj().transpose(-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     grad_A = \u001b[32m2\u001b[39m * (\u001b[43mU\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mPsi\u001b[49m) @ A @ D @ D.conj().transpose(-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_A\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (4) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "import cvxpy as cp\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Step 1: Define System and Simulation Parameters\n",
    "N = 64  # Number of BS antennas\n",
    "K = 4   # Number of users\n",
    "M = 4   # Number of RF chains\n",
    "omega = 0.3  # Tradeoff weight\n",
    "I_max = 10  # Maximum outer iterations\n",
    "J = 10  # Can be 1, 10, or 20\n",
    "\n",
    "SNR_dB = 12  # SNR in dB\n",
    "sigma_n2 = 1.0  # Noise variance\n",
    "P_BS = sigma_n2 * 10**(SNR_dB / 10)  # Transmit power\n",
    "mu = 0.01  # Step size for analog precoder\n",
    "lambda_ = 0.01  # Step size for digital precoder\n",
    "L = 20  # Number of paths for channel\n",
    "num_realizations = 2  # Number of channel realizations\n",
    "\n",
    "# Dataset parameters\n",
    "num_channels = 10\n",
    "num_epochs = 10 if J == 1 else 3\n",
    "snr_min, snr_max = 0, 12  # dB\n",
    "\n",
    "# Step 2: Define Sensing Parameters\n",
    "P = 3  # Number of desired sensing angles\n",
    "theta_d = np.array([-60, 0, 60]) * np.pi / 180  # Desired angles in radians\n",
    "delta_theta = 5 * np.pi / 180  # Half beamwidth\n",
    "theta_grid = np.linspace(-np.pi / 2, np.pi / 2, 181)  # Angular grid [-90, 90] degrees\n",
    "B_d = np.zeros(len(theta_grid))  # Desired beampattern\n",
    "for t, theta_t in enumerate(theta_grid):\n",
    "    for theta_p in theta_d:\n",
    "        if abs(theta_t - theta_p) <= delta_theta:\n",
    "            B_d[t] = 1\n",
    "\n",
    "# Wavenumber and antenna spacing\n",
    "lambda_wave = 1  # Wavelength (normalized)\n",
    "k = 2 * np.pi / lambda_wave\n",
    "d = lambda_wave / 2  # Antenna spacing\n",
    "\n",
    "# Step 3: Channel Matrix Generation (Saleh-Valenzuela Model)\n",
    "def generate_channel(N, M, L, device='cpu'):\n",
    "    H = torch.zeros((M, N), dtype=torch.cfloat, device=device)\n",
    "    for _ in range(L):\n",
    "        alpha = torch.randn(2, device=device).view(torch.cfloat)[0] / torch.sqrt(torch.tensor(2.0, device=device))\n",
    "        phi_r = torch.rand(1, device=device) * 2 * torch.pi\n",
    "        phi_t = torch.rand(1, device=device) * 2 * torch.pi\n",
    "        a_r = torch.exp(1j * k * d * torch.arange(M, dtype=torch.float32, device=device) * torch.sin(phi_r)) / torch.sqrt(torch.tensor(M, dtype=torch.float32, device=device))\n",
    "        a_t = torch.exp(1j * k * d * torch.arange(N, dtype=torch.float32, device=device) * torch.sin(phi_t)) / torch.sqrt(torch.tensor(N, dtype=torch.float32, device=device))\n",
    "        H += torch.sqrt(torch.tensor(N * M / L, dtype=torch.float32, device=device)) * alpha * torch.outer(a_r, a_t.conj())\n",
    "    return H\n",
    "\n",
    "# Steering vector function\n",
    "def steering_vector(theta, N, device='cpu'):\n",
    "    theta = torch.tensor(theta, dtype=torch.float32, device=device) if not torch.is_tensor(theta) else theta\n",
    "    return torch.exp(1j * k * d * torch.arange(N, dtype=torch.float32, device=device) * torch.sin(theta)) / torch.sqrt(torch.tensor(N, dtype=torch.float32, device=device))\n",
    "\n",
    "# Compute benchmark covariance matrix Psi\n",
    "def compute_psi(N, Bd, theta_grid, PBS):\n",
    "    Abar_grid = np.exp(1j * np.pi * np.arange(N)[:, None] @ np.sin(theta_grid)[None, :])\n",
    "    Psi = cp.Variable((N, N), hermitian=True)\n",
    "    alpha_cvx = cp.Variable(nonneg=True)\n",
    "\n",
    "    constraints = [\n",
    "        cp.diag(Psi) == (PBS / N) * np.ones(N),\n",
    "        Psi >> 0\n",
    "    ]\n",
    "    \n",
    "    Bd_vec = Bd.flatten()\n",
    "    obj = sum(cp.square(alpha_cvx * Bd_vec[t] - cp.real(cp.quad_form(Abar_grid[:, t], Psi))) for t in range(len(Bd_vec)))\n",
    "    objective = cp.Minimize(obj)\n",
    "\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    \n",
    "    try:\n",
    "        problem.solve(solver=cp.SCS, eps=1e-3)\n",
    "        if problem.status in [cp.OPTIMAL, cp.OPTIMAL_INACCURATE]:\n",
    "            Psi_optimal = Psi.value\n",
    "            alpha_optimal = alpha_cvx.value\n",
    "            return Psi_optimal, alpha_optimal\n",
    "        else:\n",
    "            print(f\"CVXPY Status: {problem.status}. Problem could not be solved to optimality.\")\n",
    "            Psi_fallback = (PBS / N) * np.eye(N, dtype=complex)\n",
    "            return Psi_fallback, None\n",
    "    except cp.error.SolverError as e:\n",
    "        print(f\"CVXPY Solver Error: {e}\")\n",
    "        print(\"Falling back to identity matrix.\")\n",
    "        Psi_fallback = (PBS / N) * np.eye(N, dtype=complex)\n",
    "        return Psi_fallback, None\n",
    "\n",
    "# Compute communication rate R\n",
    "def compute_rate(H, A, D, sigma_n2):\n",
    "    sigma_n2 = torch.tensor(sigma_n2, dtype=torch.float32, device=H.device) if not torch.is_tensor(sigma_n2) else sigma_n2\n",
    "    H_A = H @ A\n",
    "    R = torch.tensor(0.0, dtype=torch.float32, device=H.device)\n",
    "    for k in range(K):\n",
    "        h_k = H_A[k, :].reshape(-1, 1)\n",
    "        signal = torch.abs(h_k.conj().T @ D[:, k])**2\n",
    "        interference = sum(torch.abs(h_k.conj().T @ D[:, j])**2 for j in range(K) if j != k)\n",
    "        SINR = signal / (interference + sigma_n2)\n",
    "        R += torch.log2(1 + SINR)\n",
    "    return R\n",
    "\n",
    "# Compute sensing error tau\n",
    "def compute_tau(A, D, Psi, theta_grid):\n",
    "    V = A @ D\n",
    "    tau = torch.tensor(0.0, dtype=torch.float32, device=A.device)\n",
    "    theta_grid = torch.tensor(theta_grid, dtype=torch.float32, device=A.device) if not torch.is_tensor(theta_grid) else theta_grid\n",
    "    for theta in theta_grid:\n",
    "        a_theta = steering_vector(theta, A.shape[0], device=A.device)\n",
    "        a_theta = a_theta.reshape(-1, 1)\n",
    "        term1 = (a_theta.conj().T @ V @ V.conj().T @ a_theta).real\n",
    "        term2 = (a_theta.conj().T @ Psi @ a_theta).real\n",
    "        tau += torch.abs(term1 - term2)**2\n",
    "    return tau / len(theta_grid)\n",
    "\n",
    "def gradient_R_A(H, A, D, sigma_n2):\n",
    "    sigma_n2 = torch.tensor(sigma_n2, dtype=torch.float32, device=H.device) if not torch.is_tensor(sigma_n2) else sigma_n2\n",
    "    xi = 1 / torch.log(torch.tensor(2.0, dtype=A.dtype, device=A.device))\n",
    "    grad_A = torch.zeros_like(A, dtype=torch.cfloat)\n",
    "\n",
    "    V = D @ D.conj().transpose(-2, -1)\n",
    "    for k in range(K):\n",
    "        h_k = H[k, :].reshape(-1, 1)\n",
    "        H_tilde_k = h_k @ h_k.conj().transpose(-2, -1)\n",
    "        D_bar_k = D.clone()\n",
    "        D_bar_k[:, k] = 0.0\n",
    "        V_bar_k = D_bar_k @ D_bar_k.conj().transpose(-2, -1)\n",
    "        denom1 = torch.trace(A @ V @ A.conj().transpose(-2, -1) @ H_tilde_k) + sigma_n2\n",
    "        denom2 = torch.trace(A @ V_bar_k @ A.conj().transpose(-2, -1) @ H_tilde_k) + sigma_n2\n",
    "        term1 = H_tilde_k @ A @ V / denom1\n",
    "        term2 = H_tilde_k @ A @ V_bar_k / denom2\n",
    "        grad_A += xi * (term1 - term2)\n",
    "    return grad_A\n",
    "\n",
    "def gradient_R_D(H, A, D, sigma_n2):\n",
    "    sigma_n2 = torch.tensor(sigma_n2, dtype=torch.float32, device=H.device) if not torch.is_tensor(sigma_n2) else sigma_n2\n",
    "    xi = 1 / torch.log(torch.tensor(2.0, dtype=A.dtype, device=A.device))\n",
    "    grad_D = torch.zeros_like(D, dtype=torch.cfloat)\n",
    "\n",
    "    for k in range(K):\n",
    "        h_k = H[k, :].reshape(-1, 1)\n",
    "        H_tilde_k = h_k @ h_k.conj().transpose(-2, -1)\n",
    "        H_bar_k = A.conj().transpose(-2, -1) @ H_tilde_k @ A\n",
    "        D_bar_k = D.clone()\n",
    "        D_bar_k[:, k] = 0.0\n",
    "        denom1 = torch.trace(D @ D.conj().transpose(-2, -1) @ H_bar_k) + sigma_n2\n",
    "        denom2 = torch.trace(D_bar_k @ D_bar_k.conj().transpose(-2, -1) @ H_bar_k) + sigma_n2\n",
    "        term1 = (H_bar_k @ D) / denom1\n",
    "        term2 = (H_bar_k @ D_bar_k) / denom2\n",
    "        grad_D += xi * (term1 - term2)\n",
    "    return grad_D\n",
    "\n",
    "def gradient_tau_A(A, D, Psi):\n",
    "    U = A @ D @ D.conj().transpose(-2, -1) @ A.conj().transpose(-2, -1)\n",
    "    grad_A = 2 * (U - Psi) @ A @ D @ D.conj().transpose(-2, -1)\n",
    "    return grad_A\n",
    "\n",
    "def gradient_tau_D(A, D, Psi):\n",
    "    U = A @ D @ D.conj().transpose(-2, -1) @ A.conj().transpose(-2, -1)\n",
    "    grad_D = 2 * A.conj().transpose(-2, -1) @ (U - Psi) @ A @ D\n",
    "    return grad_D\n",
    "\n",
    "def proposed_initialization(H, theta_d, N, M, K, P_BS):\n",
    "    theta_d = torch.tensor(theta_d, dtype=torch.float32, device=H.device) if isinstance(theta_d, np.ndarray) else theta_d\n",
    "    G = H.T  # shape (N, K) = (64, 4)\n",
    "    A0 = torch.exp(-1j * torch.angle(G[:, :M]))  # shape (N, M) = (64, 4)\n",
    "    X_ZF = torch.linalg.pinv(H)  # shape (N, K) = (64, 4)\n",
    "    D0 = torch.linalg.pinv(A0) @ X_ZF  # shape (M, K) = (4, 4)\n",
    "    norm_factor = torch.norm(A0 @ D0, p='fro')\n",
    "    D0 = torch.sqrt(torch.tensor(P_BS, dtype=A0.dtype, device=A0.device)) * D0 / norm_factor\n",
    "    return A0, D0\n",
    "\n",
    "def random_initialization(N, M, H, P_BS, device='cpu'):\n",
    "    A0 = torch.exp(1j * torch.rand(N, M, dtype=torch.cfloat, device=device) * 2 * torch.pi)\n",
    "    D0 = torch.linalg.pinv(H @ A0)\n",
    "    norm_factor = torch.norm(A0 @ D0, p='fro')\n",
    "    D0 = torch.sqrt(torch.tensor(P_BS, dtype=A0.dtype, device=A0.device)) * D0 / norm_factor\n",
    "    return A0, D0\n",
    "\n",
    "def svd_initialization(H, N, M, K, P_BS, device='cpu'):\n",
    "    _, _, Vh = svd(H.cpu().numpy() if torch.is_tensor(H) else H, full_matrices=False)\n",
    "    A0 = torch.tensor(Vh.T[:, :M], dtype=torch.cfloat, device=device)\n",
    "    A0 = torch.exp(1j * torch.angle(A0))\n",
    "    H_A = H @ A0\n",
    "    try:\n",
    "        D0 = torch.linalg.pinv(H_A)\n",
    "    except RuntimeError:\n",
    "        D0 = torch.linalg.pinv(H_A + 1e-6 * torch.eye(M, dtype=torch.cfloat, device=device))\n",
    "    norm_factor = torch.norm(A0 @ D0, p='fro')\n",
    "    D0 = torch.sqrt(torch.tensor(P_BS, dtype=A0.dtype, device=A0.device)) * D0 / norm_factor\n",
    "    return A0, D0\n",
    "\n",
    "def project_unit_modulus(A):\n",
    "    return torch.exp(1j * torch.angle(A))\n",
    "\n",
    "def project_power_constraint(D, A, P_BS):\n",
    "    norm_factor = torch.norm(A @ D, p='fro')\n",
    "    D = D * (torch.sqrt(torch.tensor(P_BS, dtype=D.dtype, device=D.device)) / norm_factor)\n",
    "    return D\n",
    "\n",
    "def run_pga(H, A0, D0, J, I_max, mu, lambda_, omega, sigma_n2, Psi, theta_grid):\n",
    "    N, K = H.shape\n",
    "    A = A0.clone()\n",
    "    D = D0.clone()\n",
    "    objectives = []\n",
    "    eta = 1 / N\n",
    "\n",
    "    for i in range(I_max):\n",
    "        print(f\"\\n===== Outer Iteration {i+1}/{I_max} =====\")\n",
    "        A_hat = A.clone()\n",
    "        for j in range(J):\n",
    "            grad_R_A = gradient_R_A(H, A_hat, D, sigma_n2)\n",
    "            grad_tau_A = gradient_tau_A(A_hat, D, Psi)\n",
    "            grad_A = grad_R_A - omega * grad_tau_A\n",
    "            A_hat = A_hat + mu * grad_A\n",
    "            A_hat = project_unit_modulus(A_hat)\n",
    "        A = A_hat\n",
    "        grad_R_D = gradient_R_D(H, A, D, sigma_n2)\n",
    "        grad_tau_D = gradient_tau_D(A, D, Psi)\n",
    "        grad_D = grad_R_D - omega * eta * grad_tau_D\n",
    "        D = D + lambda_ * grad_D\n",
    "        D = project_power_constraint(D, A, P_BS)\n",
    "        R = compute_rate(H, A, D, sigma_n2)\n",
    "        tau = compute_tau(A, D, Psi, theta_grid)\n",
    "        objective = R - omega * tau\n",
    "        objectives.append(objective)\n",
    "        print(f\"Iteration {i+1}: R = {R:.4f}, τ = {tau:.4e}, Objective = {objective:.4f}\")\n",
    "\n",
    "    return objectives\n",
    "\n",
    "class UPGANetLayer(nn.Module):\n",
    "    def __init__(self, N, M, K, omega, J=10, eta=None):\n",
    "        super(UPGANetLayer, self).__init__()\n",
    "        self.J = J\n",
    "        self.N, self.M, self.K = N, M, K\n",
    "        self.omega = omega\n",
    "        self.eta = eta if eta is not None else 1/N\n",
    "        self.mu = nn.Parameter(torch.full((J,), 0.01, dtype=torch.float32))\n",
    "        self.lambda_ = nn.Parameter(torch.tensor(0.01, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, H, A, D, Psi, sigma_n2, P_BS):\n",
    "        sigma_n2 = torch.tensor(sigma_n2, dtype=torch.float32, device=H.device) if not torch.is_tensor(sigma_n2) else sigma_n2\n",
    "        P_BS = torch.tensor(P_BS, dtype=torch.float32, device=H.device) if not torch.is_tensor(P_BS) else P_BS\n",
    "        for j in range(self.J):\n",
    "            grad_RA = gradient_R_A(H, A, D, sigma_n2)\n",
    "            grad_tauA = gradient_tau_A(A, D, Psi)\n",
    "            A = A + self.mu[j] * (grad_RA - self.omega * grad_tauA)\n",
    "            A = project_unit_modulus(A)\n",
    "        grad_RD = gradient_R_D(H, A, D, sigma_n2)\n",
    "        grad_tauD = gradient_tau_D(A, D, Psi)\n",
    "        D = D + self.lambda_ * (grad_RD - self.omega * self.eta * grad_tauD)\n",
    "        D = project_power_constraint(D, A, P_BS)\n",
    "        return A, D\n",
    "\n",
    "class UPGANet(nn.Module):\n",
    "    def __init__(self, N, M, K, omega, I_max=120, J=10):\n",
    "        super(UPGANet, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            UPGANetLayer(N, M, K, omega, J=J) for _ in range(I_max)\n",
    "        ])\n",
    "        self.I_max = I_max\n",
    "        self.omega = omega\n",
    "\n",
    "    def forward(self, H, A0, D0, Psi, sigma_n2, P_BS):\n",
    "        A, D = A0, D0\n",
    "        for i in range(self.I_max):\n",
    "            A, D = self.layers[i](H, A, D, Psi, sigma_n2, P_BS)\n",
    "        return A, D\n",
    "\n",
    "def upganet_loss(H, A, D, Psi, sigma_n2, omega):\n",
    "    sigma_n2 = torch.tensor(sigma_n2, dtype=torch.float32, device=H.device) if not torch.is_tensor(sigma_n2) else sigma_n2\n",
    "    omega = torch.tensor(omega, dtype=torch.float32, device=H.device) if not torch.is_tensor(omega) else omega\n",
    "    R = compute_rate(H, A, D, sigma_n2)\n",
    "    tau = compute_tau(A, D, Psi, theta_grid)\n",
    "    return -(R - omega * tau)\n",
    "\n",
    "# Instantiate model\n",
    "model = UPGANet(N, M, K, omega, I_max=I_max, J=J)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for _ in range(num_channels):\n",
    "        H = generate_channel(N, M, L=3, device=device).transpose(0, 1)\n",
    "        snr_db = np.random.uniform(snr_min, snr_max)\n",
    "        P_BS = 10 ** (snr_db / 10)\n",
    "        Psi, alpha_opt = compute_psi(N, B_d, theta_grid, P_BS)\n",
    "        Psi = np.array(Psi, dtype=np.complex64)\n",
    "        H_t = H.to(device)\n",
    "        Psi_t = torch.tensor(Psi, dtype=torch.cfloat, device=device)\n",
    "        theta_d_t = torch.tensor(theta_d, dtype=torch.float32, device=device)\n",
    "        A0, D0 = proposed_initialization(H_t, theta_d_t, N, M, K, P_BS)\n",
    "        A0_t = A0.to(device)\n",
    "        D0_t = D0.to(device)\n",
    "        A_final, D_final = model(H_t, A0_t, D0_t, Psi_t, sigma_n2, P_BS)\n",
    "        loss = upganet_loss(H_t, A_final, D_final, Psi_t, sigma_n2, omega)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[Epoch {epoch+1}/{num_epochs}] Loss: {total_loss/num_channels:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
